"""
Morningstar è²¡å ±æœƒè­°ç´€éŒ„çˆ¬èŸ² (Linux/WSL/Windows é€šç”¨ç‰ˆ)
===========================
å¾ Morningstar ç¶²ç«™çˆ¬å–å…¬å¸çš„è²¡å ±æœƒè­°ç´€éŒ„ï¼ˆEarnings Transcriptsï¼‰
ä¸¦æ”¯æ´å¯«å…¥ PostgreSQL è³‡æ–™åº«
"""

import csv
import json
import time
import random
import re
import os
import sys
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException

# [æ–°å¢] å¼•å…¥ webdriver_manager è‡ªå‹•ç®¡ç†é©…å‹•
from webdriver_manager.chrome import ChromeDriverManager

# [æ–°å¢] å¼•å…¥ psycopg2 ç”¨æ–¼é€£æ¥ PostgreSQL
import psycopg2


class EarningsScraper:
    """è²¡å ±æœƒè­°ç´€éŒ„çˆ¬èŸ²é¡åˆ¥"""

    def __init__(self):
        """
        åˆå§‹åŒ–çˆ¬èŸ²
        """
        self.results = []
        self.driver = None
        self.db_conn = None

        # [æ–°å¢] åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥
        self.init_db()

    def init_db(self):
        """åˆå§‹åŒ– PostgreSQL è³‡æ–™åº«é€£æ¥ä¸¦å»ºç«‹è³‡æ–™è¡¨ (Schema: morningstar)"""
        try:
            # å¾ç’°å¢ƒè®Šæ•¸è®€å–é€£ç·šè³‡è¨Š
            db_host = os.getenv("DB_POSTGRESDB_HOST", "postgres")
            db_name = os.getenv("DB_POSTGRESDB_DATABASE", "n8n")
            db_user = os.getenv("DB_POSTGRESDB_USER", "n8n")
            db_pass = os.getenv("DB_POSTGRESDB_PASSWORD", "n8n")
            db_port = os.getenv("DB_POSTGRESDB_PORT", "5432")

            print(f" ğŸ˜ æ­£åœ¨é€£æ¥ PostgreSQL ({db_host}:{db_port})...")

            self.db_conn = psycopg2.connect(
                host=db_host,
                database=db_name,
                user=db_user,
                password=db_pass,
                port=db_port,
            )
            self.db_conn.autocommit = True

            with self.db_conn.cursor() as cursor:
                # [æ–°å¢] 1. å»ºç«‹ Schema (å¦‚æœä¸å­˜åœ¨)
                cursor.execute("CREATE SCHEMA IF NOT EXISTS morningstar;")

                # [ä¿®æ”¹] 2. å»ºç«‹è³‡æ–™è¡¨ (åŠ ä¸Š schema å‰ç¶´)
                create_table_query = """
                CREATE TABLE IF NOT EXISTS morningstar.earnings_transcripts (
                    id SERIAL PRIMARY KEY,
                    ticker VARCHAR(50) NOT NULL,
                    company_name VARCHAR(255),
                    quarter VARCHAR(50) NOT NULL,
                    transcript TEXT,
                    date DATE,
                    url TEXT,
                    scraped_at TIMESTAMP,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(ticker, quarter)
                );
                """
                cursor.execute(create_table_query)
                print(" âœ… è³‡æ–™åº«è³‡æ–™è¡¨æª¢æŸ¥å®Œæˆ (Schema: morningstar)")

        except Exception as e:
            print(f" âš ï¸ è³‡æ–™åº«é€£æ¥å¤±æ•— (å°‡åªå„²å­˜ JSON): {e}")
            self.db_conn = None

    @staticmethod
    def get_quarter_from_date(date_string):
        """
        æ ¹æ“šæ—¥æœŸå­—ä¸²åˆ¤æ–·å­£åº¦

        Args:
            date_string: æ—¥æœŸå­—ä¸²ï¼ˆä»»ä½•æ ¼å¼ï¼‰

        Returns:
            str: å­£åº¦æ¨™ç±¤ï¼ˆæ ¼å¼ï¼šYYYY_Q#ï¼‰æˆ– "Unknown"
        """
        if not date_string or date_string == "Unknown":
            return "Unknown"

        try:
            # å…ˆæ ¼å¼åŒ–æ—¥æœŸ
            formatted_date = EarningsScraper.format_date(date_string)

            if formatted_date == "Unknown" or "/" not in formatted_date:
                return "Unknown"

            # è§£æ YYYY/MM/DD æ ¼å¼
            parts = formatted_date.split("/")
            if len(parts) != 3:
                return "Unknown"

            year = parts[0]
            month = int(parts[1])

            # æ ¹æ“šæœˆä»½åˆ¤æ–·å­£åº¦
            if 1 <= month <= 3:
                quarter = "Q1"
            elif 4 <= month <= 6:
                quarter = "Q2"
            elif 7 <= month <= 9:
                quarter = "Q3"
            elif 10 <= month <= 12:
                quarter = "Q4"
            else:
                return "Unknown"

            return f"{year}_{quarter}"

        except Exception as e:
            print(f"    âš ï¸ åˆ¤æ–·å­£åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return "Unknown"

    @staticmethod
    def format_date(date_string):
        """
        å°‡æ—¥æœŸå­—ä¸²æ ¼å¼åŒ–ç‚º YYYY/MM/DD æ ¼å¼

        Args:
            date_string: åŸå§‹æ—¥æœŸå­—ä¸²

        Returns:
            str: æ ¼å¼åŒ–å¾Œçš„æ—¥æœŸå­—ä¸² (YYYY/MM/DD) æˆ–åŸå§‹å­—ä¸²
        """
        if not date_string or date_string == "Unknown":
            return "Unknown"

        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½
        date_string = date_string.strip()

        # æœˆä»½åç¨±å°æ‡‰
        month_map = {
            "jan": "01",
            "january": "01",
            "feb": "02",
            "february": "02",
            "mar": "03",
            "march": "03",
            "apr": "04",
            "april": "04",
            "may": "05",
            "jun": "06",
            "june": "06",
            "jul": "07",
            "july": "07",
            "aug": "08",
            "august": "08",
            "sep": "09",
            "september": "09",
            "oct": "10",
            "october": "10",
            "nov": "11",
            "november": "11",
            "dec": "12",
            "december": "12",
        }

        # å˜—è©¦ç›´æ¥ç”¨æ­£å‰‡æ›¿æ› (YYYY-MM-DD æˆ– YYYY/MM/DD)
        iso_match = re.match(r"(\d{4})[-/](\d{1,2})[-/](\d{1,2})", date_string)
        if iso_match:
            year, month, day = iso_match.groups()
            return f"{year}/{month.zfill(2)}/{day.zfill(2)}"

        # å˜—è©¦è§£æç¾å¼æ ¼å¼ (Jan 31, 2024)
        us_match = re.search(
            r"(Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\s+(\d{1,2}),?\s+(\d{4})",
            date_string,
            re.IGNORECASE,
        )
        if us_match:
            month_name, day, year = us_match.groups()
            month_num = month_map.get(month_name.lower(), "00")
            return f"{year}/{month_num}/{day.zfill(2)}"

        # å˜—è©¦è§£æç¾å¼ç°¡çŸ­æ ¼å¼ (MM/DD/YYYY)
        us_short_match = re.match(r"(\d{1,2})/(\d{1,2})/(\d{4})", date_string)
        if us_short_match:
            month, day, year = us_short_match.groups()
            return f"{year}/{month.zfill(2)}/{day.zfill(2)}"

        # å˜—è©¦è§£ææ­å¼æ ¼å¼ (31 Jan 2024)
        eu_match = re.search(
            r"(\d{1,2})[-\s](Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)[-\s](\d{4})",
            date_string,
            re.IGNORECASE,
        )
        if eu_match:
            day, month_name, year = eu_match.groups()
            month_num = month_map.get(month_name.lower(), "00")
            return f"{year}/{month_num}/{day.zfill(2)}"

        # å¦‚æœéƒ½ç„¡æ³•è§£æï¼Œå˜—è©¦ä½¿ç”¨ datetime.strptime
        try:
            # å˜—è©¦å¤šç¨®æ ¼å¼
            for fmt in [
                "%Y-%m-%d",
                "%Y/%m/%d",
                "%d-%m-%Y",
                "%d/%m/%Y",
                "%m-%d-%Y",
                "%m/%d/%Y",
                "%B %d, %Y",
                "%b %d, %Y",
                "%d %B %Y",
                "%d %b %Y",
            ]:
                try:
                    dt = datetime.strptime(date_string, fmt)
                    return dt.strftime("%Y/%m/%d")
                except ValueError:
                    continue
        except:
            pass

        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—ï¼Œè¿”å›åŸå§‹å­—ä¸²
        return date_string

    def setup_driver(self):
        """è¨­å®š Chrome WebDriver (è‡ªå‹•é©é… Windows/Linux/WSL)"""
        options = webdriver.ChromeOptions()

        # è¨­å®š User-Agent æ¨¡æ“¬çœŸå¯¦ç€è¦½å™¨
        options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )

        # [é‡è¦] Linux/WSL ç’°å¢ƒè¨­å®š
        # åœ¨ä¼ºæœå™¨æˆ– WSL ç’°å¢ƒä¸‹ï¼Œé€šå¸¸å¿…é ˆä½¿ç”¨ headless æ¨¡å¼
        if sys.platform.startswith("linux"):
            print(" ğŸ§ åµæ¸¬åˆ° Linux ç’°å¢ƒï¼Œå•Ÿç”¨ Headless æ¨¡å¼èˆ‡ Sandbox ä¿®è£œ")
            options.add_argument("--headless=new")  # æ–°ç‰ˆç„¡é ­æ¨¡å¼
            options.add_argument("--no-sandbox")  # è§£æ±ºæ¬Šé™å•é¡Œ
            options.add_argument("--disable-dev-shm-usage")  # è§£æ±ºè¨˜æ†¶é«”å…±äº«å•é¡Œ
            options.add_argument("--disable-gpu")
            options.add_argument("--window-size=1920,1080")  # è¨­å®šè¦–çª—å¤§å°ä»¥ç¢ºä¿å…ƒç´ æ¸²æŸ“
        else:
            # Windows é–‹ç™¼éšæ®µï¼šå¯é¸æ“‡æ˜¯å¦é–‹å•Ÿ headless
            # options.add_argument('--headless')
            pass

        options.add_argument("--disable-blink-features=AutomationControlled")

        try:
            # [ä¿®æ”¹] ä½¿ç”¨ ChromeDriverManager è‡ªå‹•å®‰è£èˆ‡è¼‰å…¥é©…å‹•
            driver_path = ChromeDriverManager().install()
            print(f" ğŸ”§ WebDriver è·¯å¾‘: {driver_path}")

            service = Service(driver_path)
            self.driver = webdriver.Chrome(service=service, options=options)

            if not sys.platform.startswith("linux"):
                self.driver.maximize_window()

        except Exception as e:
            print(f"âŒ WebDriver åˆå§‹åŒ–å¤±æ•—: {e}")
            print("ğŸ’¡ æç¤º: è«‹ç¢ºèªç³»çµ±å·²å®‰è£ Google Chrome ç€è¦½å™¨")
            if sys.platform.startswith("linux"):
                print(
                    "   Linux å®‰è£æŒ‡ä»¤: wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb && sudo dpkg -i google-chrome-stable_current_amd64.deb && sudo apt-get install -f -y"
                )
            raise e

    def read_urls_from_csv(self, csv_path):
        """
        å¾ CSV æª”æ¡ˆè®€å–ç¶²å€åˆ—è¡¨
        """
        urls = []
        if not os.path.exists(csv_path):
            print(f"âŒ æ‰¾ä¸åˆ° CSV æª”æ¡ˆ: {csv_path}")
            return []

        try:
            with open(csv_path, "r", encoding="utf-8") as file:
                reader = csv.reader(file)
                for row in reader:
                    if row:  # ç¢ºä¿ä¸æ˜¯ç©ºè¡Œ
                        urls.append(row[0].strip())
            print(f"âœ… æˆåŠŸè®€å– {len(urls)} å€‹ç¶²å€")
            return urls
        except Exception as e:
            print(f"âŒ è®€å– CSV æª”æ¡ˆå¤±æ•—: {e}")
            return []

    def debug_page_buttons(self):
        """èª¿è©¦æ–¹æ³•ï¼šé¡¯ç¤ºé é¢ä¸Šæ‰€æœ‰çš„æŒ‰éˆ•å’Œå¯é»æ“Šå…ƒç´ """
        try:
            print("\n  ğŸ” === èª¿è©¦ï¼šé é¢å…ƒç´ åˆ†æ ===")

            # å°‹æ‰¾æ‰€æœ‰æŒ‰éˆ•
            buttons = self.driver.find_elements(By.TAG_NAME, "button")
            print(f"  ğŸ“Œ æ‰¾åˆ° {len(buttons)} å€‹ button å…ƒç´ :")
            for i, btn in enumerate(buttons[:5], 1):  # åªé¡¯ç¤ºå‰5å€‹é¿å…æ´—ç‰ˆ
                text = btn.text.strip() or "(ç„¡æ–‡å­—)"
                classes = btn.get_attribute("class") or "(ç„¡class)"
                print(f"     {i}. æ–‡å­—: '{text}' | class: '{classes}'")

            # å°‹æ‰¾æ‰€æœ‰æ—¥æœŸé¸é … labels
            date_labels = self.driver.find_elements(By.CSS_SELECTOR, "label.mds-radio-button__sal")
            print(f"  ğŸ“Œ æ‰¾åˆ° {len(date_labels)} å€‹æ—¥æœŸé¸é … labels")

            print("  ğŸ” === èª¿è©¦çµæŸ ===\n")
        except Exception as e:
            print(f"  âš ï¸ èª¿è©¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def collect_all_transcripts_by_clicking_dates(self, company_name, ticker, url):
        """é€ä¸€é»æ“Šæ‰€æœ‰æ—¥æœŸé¸é …ä¸¦ç›´æ¥æå–é€å­—ç¨¿å…§å®¹"""
        all_transcripts = []

        try:
            print("  ğŸ“… å°‹æ‰¾ä¸¦æ‰“é–‹æ—¥æœŸé¸æ“‡é¸å–®...")

            # å˜—è©¦é»æ“Š"ç™¼è¡¨æ—¥æœŸ"æŒ‰éˆ•
            date_button_selectors = [
                "//button[contains(., 'Published Date')]",
                "//span[contains(text(), 'Published Date')]/..",
                "button[class*='event']",
                "#eventPopver-transcript",
            ]

            for selector in date_button_selectors:
                try:
                    if selector.startswith("//"):
                        btn = self.driver.find_element(By.XPATH, selector)
                    else:
                        btn = self.driver.find_element(By.CSS_SELECTOR, selector)

                    self.driver.execute_script("arguments[0].click();", btn)
                    time.sleep(2)
                    break
                except:
                    continue

            # å°‹æ‰¾æ‰€æœ‰æ—¥æœŸé¸é …
            label_selectors = [
                "label.mds-radio-button__sal",
                'label[class*="mds-radio-button"]',
            ]

            date_labels = []
            for selector in label_selectors:
                try:
                    date_labels = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if date_labels:
                        print(f"  âœ… æ‰¾åˆ° {len(date_labels)} å€‹æ—¥æœŸé¸é …")
                        break
                except:
                    continue

            if not date_labels:
                print("  âš ï¸ æœªæ‰¾åˆ°å¤šå€‹æ—¥æœŸé¸é …ï¼ŒæŠ“å–ç•¶å‰é é¢")
                transcript_data = self.extract_transcript_data(url, company_name, ticker, "TBD")
                if transcript_data:
                    all_transcripts.append(transcript_data)
                return all_transcripts

            print(f"  ğŸ”„ å°‡é€ä¸€é»æ“Š {len(date_labels)} å€‹æ—¥æœŸé¸é …...")

            # é€ä¸€é»æ“Šæ¯å€‹ label
            for idx, label in enumerate(date_labels, 1):
                try:
                    # ç‚ºäº†é¿å…å…ƒç´ éæœŸ (StaleElement)ï¼Œæ¯æ¬¡é‡æ–°æŠ“å–åˆ—è¡¨
                    current_labels = self.driver.find_elements(
                        By.CSS_SELECTOR, "label.mds-radio-button__sal"
                    )
                    if idx - 1 < len(current_labels):
                        current_label = current_labels[idx - 1]

                        # ä½¿ç”¨ JS é»æ“Šæœ€ç©©å®š
                        self.driver.execute_script("arguments[0].click();", current_label)
                        print(f"    [{idx}] é»æ“Šæ—¥æœŸé¸é …...")

                        time.sleep(3)  # ç­‰å¾…æ¸²æŸ“

                        transcript_data = self.extract_transcript_data(
                            url, company_name, ticker, "TBD"
                        )
                        if transcript_data:
                            all_transcripts.append(transcript_data)
                            print(
                                f"    âœ… æå–æˆåŠŸ (é•·åº¦: {len(transcript_data.get('transcript', ''))})"
                            )
                except Exception as e:
                    print(f"    âš ï¸ è™•ç†ç¬¬ {idx} å€‹æ—¥æœŸæ™‚éŒ¯èª¤: {e}")
                    continue

            return all_transcripts

        except Exception as e:
            print(f"  âŒ æ”¶é›†é€å­—ç¨¿æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            transcript_data = self.extract_transcript_data(url, company_name, ticker, "TBD")
            if transcript_data:
                all_transcripts.append(transcript_data)
            return all_transcripts

    def scrape_transcript_page(self, url, output_dir):
        """çˆ¬å–å–®ä¸€è²¡å ±æœƒè­°ç´€éŒ„é é¢"""
        page_results = []

        try:
            print(f"\nğŸŒ æ­£åœ¨è¨ªå•: {url}")
            self.driver.get(url)

            wait = WebDriverWait(self.driver, 20)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.company-name")))
            time.sleep(3)

            # æå–åŸºæœ¬è³‡è¨Š
            try:
                company_name = self.driver.find_element(
                    By.CSS_SELECTOR, "div.company-name"
                ).text.strip()
            except:
                company_name = "Unknown"

            try:
                ticker = self.driver.find_element(By.CSS_SELECTOR, "span.ticker").text.strip()
            except:
                ticker = "Unknown"

            print(f"ğŸ“Š å…¬å¸: {company_name} ({ticker})")

            if "/earnings-transcripts" in url or "/transcript" in url:
                print("\n  ğŸ’¡ æª¢æ¸¬åˆ°é€å­—ç¨¿é é¢ï¼Œé–‹å§‹è™•ç†...")
                page_results = self.collect_all_transcripts_by_clicking_dates(
                    company_name, ticker, url
                )
            else:
                print("  âš ï¸ éé€å­—ç¨¿é é¢")
                return page_results

            # åˆ†é¡ä¸¦å„²å­˜ (æ”¹ç‚ºåŒæ­¥å¯«å…¥ DB)
            print(f"\nğŸ“… æ­£åœ¨åˆ†é¡ä¸¦å„²å­˜è³‡æ–™...")
            self.classify_and_save_by_quarter(page_results, ticker, company_name, output_dir)

            print(f"âœ… å®Œæˆ {url}")

        except TimeoutException:
            print(f"âŒ é é¢è¼‰å…¥é€¾æ™‚: {url}")
        except Exception as e:
            print(f"âŒ çˆ¬å–é é¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        return page_results

    def extract_transcript_data(
        self, transcript_url, company_name, ticker, quarter_label="Unknown"
    ):
        """æå–é€å­—ç¨¿è³‡æ–™"""
        try:
            # ç¢ºä¿å…ƒç´ å­˜åœ¨
            try:
                transcript_element = self.driver.find_element(By.CSS_SELECTOR, "div.transcript")
                transcript_text = transcript_element.text.strip()
            except NoSuchElementException:
                return None

            # æå–æ—¥æœŸ
            date_text = "Unknown"
            date_selectors = [
                "#eventPopver-transcript > span:nth-child(2)",
                "span.date",
                "time",
            ]
            for selector in date_selectors:
                try:
                    date_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    date_text = date_element.text.strip()
                    if date_text:
                        break
                except:
                    continue

            formatted_date = self.format_date(date_text)

            data = {
                "company_name": company_name,
                "ticker": ticker,
                "quarter": quarter_label,
                "transcript": transcript_text,
                "date": formatted_date,
                "url": transcript_url,
                "scraped_at": datetime.now().strftime("%Y/%m/%d %H:%M:%S"),
            }
            return data

        except Exception as e:
            print(f"    âŒ æå–å¤±æ•—: {e}")
            return None

    def scrape_all(self, csv_path, output_dir="output"):
        """åŸ·è¡Œå®Œæ•´çˆ¬èŸ²æµç¨‹"""
        print("=" * 60)
        print("ğŸš€ é–‹å§‹åŸ·è¡Œ Morningstar è²¡å ±æœƒè­°ç´€éŒ„çˆ¬èŸ² (DB Ready)")
        print("=" * 60)

        urls = self.read_urls_from_csv(csv_path)
        if not urls:
            print("âŒ æ²’æœ‰ç¶²å€ï¼Œç¨‹å¼çµæŸ")
            return

        print("\nğŸ”§ æ­£åœ¨åˆå§‹åŒ– WebDriver...")
        self.setup_driver()

        try:
            for idx, url in enumerate(urls, 1):
                print(f"\nğŸ“ é€²åº¦: [{idx}/{len(urls)}]")
                page_results = self.scrape_transcript_page(url, output_dir)
                self.results.extend(page_results)

                if idx < len(urls):
                    delay = random.uniform(3, 5)
                    print(f"\nâ³ ä¼‘çœ  {delay:.1f} ç§’...")
                    time.sleep(delay)

            self.show_final_stats()

        except KeyboardInterrupt:
            print("\n\nâš ï¸ ä½¿ç”¨è€…ä¸­æ–·ç¨‹å¼")
            self.show_final_stats()
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            if self.driver:
                print("\nğŸ”§ é—œé–‰ç€è¦½å™¨...")
                self.driver.quit()
            if self.db_conn:
                print("ğŸ˜ é—œé–‰è³‡æ–™åº«é€£ç·š...")
                self.db_conn.close()

    def classify_and_save_by_quarter(self, all_data, ticker, company_name, output_dir):
        """åˆ†é¡ä¸¦å„²å­˜ (JSON + DB)"""
        if not all_data:
            return

        quarters_data = {}
        for record in all_data:
            date_str = record.get("date", "Unknown")
            quarter_label = self.get_quarter_from_date(date_str)
            record["quarter"] = quarter_label

            if quarter_label not in quarters_data:
                quarters_data[quarter_label] = []
            quarters_data[quarter_label].append(record)

        for quarter_label, quarter_records in quarters_data.items():
            self.save_quarter_results(
                quarter_records, ticker, company_name, quarter_label, output_dir
            )

    def save_quarter_results(self, quarter_data, ticker, company_name, quarter_label, output_dir):
        """å„²å­˜ JSON ä¸¦å¯«å…¥è³‡æ–™åº«"""
        if not quarter_data:
            return

        # 1. å„²å­˜ç‚º JSON (ä¿ç•™åŸåŠŸèƒ½ä½œç‚ºå‚™ä»½)
        try:
            os.makedirs(output_dir, exist_ok=True)
            safe_ticker = re.sub(r"[^\w\-]", "_", ticker)
            safe_quarter = re.sub(r"[^\w\-]", "_", quarter_label)
            filename = f"{safe_ticker}_{safe_quarter}.json"
            output_path = os.path.join(output_dir, filename)

            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(quarter_data, f, ensure_ascii=False, indent=2)

            print(f"  ğŸ’¾ JSON å·²å„²å­˜: {filename}")
        except Exception as e:
            print(f"  âŒ JSON å­˜æª”å¤±æ•—: {e}")

        # 2. å¯«å…¥ Postgres è³‡æ–™åº«
        if self.db_conn:
            try:
                with self.db_conn.cursor() as cursor:
                    for record in quarter_data:
                        # è™•ç†æ—¥æœŸæ ¼å¼ä»¥ç¬¦åˆ SQL DATE (YYYY-MM-DD)
                        sql_date = (
                            record["date"].replace("/", "-")
                            if record["date"] != "Unknown"
                            else None
                        )

                        insert_query = """
                        INSERT INTO morningstar.earnings_transcripts 
                        (ticker, company_name, quarter, transcript, date, url, scraped_at)
                        VALUES (%s, %s, %s, %s, %s, %s, %s)
                        ON CONFLICT (ticker, quarter) 
                        DO UPDATE SET 
                            transcript = EXCLUDED.transcript,
                            scraped_at = EXCLUDED.scraped_at,
                            url = EXCLUDED.url;
                        """
                        cursor.execute(
                            insert_query,
                            (
                                record["ticker"],
                                record["company_name"],
                                record["quarter"],
                                record["transcript"],
                                sql_date,
                                record["url"],
                                datetime.now(),
                            ),
                        )
                print(f"  ğŸ˜ DB å·²å¯«å…¥/æ›´æ–°: {len(quarter_data)} ç­†è³‡æ–™")
            except Exception as e:
                print(f"  âŒ DB å¯«å…¥å¤±æ•—: {e}")

    def show_final_stats(self):
        """é¡¯ç¤ºçµ±è¨ˆ"""
        if not self.results:
            print("\nğŸ“Š ç„¡è³‡æ–™")
            return
        print(f"\nğŸ“Š ç¸½è¨ˆçˆ¬å–: {len(self.results)} ç­†è³‡æ–™")


def main():
    """ä¸»ç¨‹å¼"""
    # è¨­å®šè·¯å¾‘
    BASE_DIR = os.getcwd()

    # [ä¿®æ”¹] 1. æ”¹è®€å– input è³‡æ–™å¤¾ï¼Œä¸¦æ›´æ–°æª”å
    # åŸæœ¬: os.path.join(BASE_DIR, "source", "weburl.csv")
    CSV_PATH = os.path.join(BASE_DIR, "input", "morningstar_ET_urls.csv")

    # [ä¿®æ”¹] 2. æ›´æ–°è¼¸å‡ºè·¯å¾‘åˆ°å­è³‡æ–™å¤¾ morningstar_ET
    # åŸæœ¬: os.path.join(BASE_DIR, "output")
    OUTPUT_DIR = os.path.join(BASE_DIR, "output", "morningstar_ET")

    # å»ºç«‹çˆ¬èŸ²å¯¦ä¾‹
    scraper = EarningsScraper()
    scraper.scrape_all(CSV_PATH, OUTPUT_DIR)


if __name__ == "__main__":
    main()
